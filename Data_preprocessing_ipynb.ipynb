{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data preprocessing. ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import seaborn as sns \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter \n",
        "import re\n",
        "import string\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "UzNa_efdUIAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\") \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Hibp2BPjp3zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**read data**"
      ],
      "metadata": {
        "id": "Lkt2OkyUOMyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dialect_dataset=pd.read_csv(\"/content/drive/MyDrive/all_dialect_dataset.csv\",lineterminator='\\n').drop(\"id.1\",axis=1)"
      ],
      "metadata": {
        "id": "LLohopESVadq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_dialect_dataset.shape)\n",
        "all_dialect_dataset.head(5)"
      ],
      "metadata": {
        "id": "xyNj99naO0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dialect_dataset[\"dialect\"].nunique()"
      ],
      "metadata": {
        "id": "tkdrrfXyAc8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(all_dialect_dataset.isnull().any(axis=1)) # there is no null values "
      ],
      "metadata": {
        "id": "tb28bHgMPfSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**"
      ],
      "metadata": {
        "id": "lB0eu5aOqbiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data['clean_tweet'] = Data.txt.str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE) #removes emojis\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('@[_A-Za-z0-9]+', '') #removes handles\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('RT', '') #removes rt\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('#',' ') #removes hashtag symbol only\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True) #removes links\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('\\d+', '') #removes numbers\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('\\n', ' ') #removes new line\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('_', '') #removes underscore\n",
        "\n",
        "Data['clean_tweet'] = Data.clean_tweet.str.replace('[^\\w\\s]','') #removes punctuation"
      ],
      "metadata": {
        "id": "8Ppx97ePpwEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizearabic\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "Data['clean_tweet'] = Data['clean_tweet'].apply(normalize_arabic)   "
      ],
      "metadata": {
        "id": "vLUZ4qXwqKO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove repeating characters\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "Data['clean_tweet'] = Data['clean_tweet'].apply(remove_repeating_char)   "
      ],
      "metadata": {
        "id": "kFvl-JyfqOK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove Non Arabic characters\n",
        "def removeNonArabicChar(text):\n",
        "    return re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+', ' ', text)\n",
        "Data['clean_tweet'] = Data['clean_tweet'].apply(removeNonArabicChar)   "
      ],
      "metadata": {
        "id": "-OR6g9WorCs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove Unnecessary Spaces\n",
        "def removeUnnecessarySpaces(text):\n",
        "    return re.sub(r'[\\n\\t\\ ]+', ' ', text)\n",
        "Data['clean_tweet'] = Data['clean_tweet'].apply(removeUnnecessarySpaces)"
      ],
      "metadata": {
        "id": "JtX3inqkrKYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.dialect.value_counts(normalize=True)*100 # imbalanced data problem"
      ],
      "metadata": {
        "id": "j4pgbaVorVA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\")) #arabic stopwords are not biult-in, so we find them by calling a set object\n",
        "\n",
        "def delete_stop_words(mess):\n",
        "\n",
        "    extra=[\"من\",\"الله\",\"اله\",\"كى\",\"رب\",\"خلا\",\"عدا\",\"حاشا\",\"عن\",\"حتي\",\"و\",\"فى\",\"انا\",\"علي\",\"الى\",\"انت\",\"الي\",\"انتما\",\"واله\",\"مش\",\"ان\",\"او\",\"اذا\",\"انتم\",\"وانا\",\"انتن\",\"هذا\",\"هذه\",\"هؤلاء\",\"هم\",\"هن\",\"هى\",\"هو\"]\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"arabic\")+extra)\n",
        "    \n",
        "    return ' '.join(word for word in mess.split() if word not in stop_words and len(word)>1)\n",
        "    \n",
        "Data['clean_tweet'] = Data['clean_tweet'].apply(delete_stop_words)"
      ],
      "metadata": {
        "id": "Mh_o9sdirgVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arb_stopwords)"
      ],
      "metadata": {
        "id": "2LDpLwbNr70S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data['clean_tweet'].astype(str)\n",
        "target = Data['dialect'].astype('category')\n",
        "\n",
        "# Split dataset into training set and test set before feature extraction\n",
        "X_train, X_test, y_train, y_test = train_test_split(data,target, test_size=0.3,random_state=109, shuffle=True,stratify=target) "
      ],
      "metadata": {
        "id": "TfK2vx6KsKwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR ML MODEL"
      ],
      "metadata": {
        "id": "BvfXYZVH3UIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extractin starts with counting the unique features\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "\n",
        "#then transforms the features\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer(use_idf=False)\n",
        "X_train_tf = tf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tf.shape"
      ],
      "metadata": {
        "id": "XX7fG1rX3hLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train_tf\n",
        "labels = y_train\n",
        "features.shape"
      ],
      "metadata": {
        "id": "4vsJEsrV3uag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for deep learning model"
      ],
      "metadata": {
        "id": "vWtNyvf55Czd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 50\n",
        "vocab_size = 100000\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "dict(list(word_index.items())[0:10])"
      ],
      "metadata": {
        "id": "KxnOe8pa3v8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "print(train_sequences[10])"
      ],
      "metadata": {
        "id": "03aOCRTO5HCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating=trunc_type)\n",
        "print(len(train_sequences[0]))\n",
        "print(len(train_padded[0]))\n",
        "\n",
        "print(len(train_sequences[1]))\n",
        "print(len(train_padded[1]))\n",
        "\n",
        "print(len(train_sequences[10]))\n",
        "print(len(train_padded[10]))"
      ],
      "metadata": {
        "id": "h9e1tHC95OQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)"
      ],
      "metadata": {
        "id": "n9rJwrOx5Obb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(target)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(y_train))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(y_test))\n",
        "print(training_label_seq[0])\n",
        "print(training_label_seq[1])\n",
        "print(training_label_seq[2])\n",
        "print(training_label_seq.shape)\n",
        "\n",
        "print(validation_label_seq[0])\n",
        "print(validation_label_seq[1])\n",
        "print(validation_label_seq[2])\n",
        "print(validation_label_seq.shape)"
      ],
      "metadata": {
        "id": "oRpTnPqS5Sb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}